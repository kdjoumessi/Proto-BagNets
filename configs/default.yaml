base:
  sample: null
  device: 'cuda:0'
  random_seed: 0

paths:
  root: "/gpfs01/berens/data/data/kermani_oct/CellData" # main_folder
  dset_dir: "OCT_preprocessed_resized_3_channels_496"
  save: 'Outputs'
  model_dir: "ProtoPNet" # ProtoBagNet
  model_save_path: null
  train_csv: 'train_normal_drusen.csv' 
  val_csv: 'val_normal_drusen.csv' 
  test_csv: 'test_normal_drusen.csv' 

data:
  binary: True #False
  target_col_name: 'drusen_vs_healthy'
  num_classes: 2 
  threshold: Falsse
  mean: [0.19075624644756317, 0.19075624644756317, 0.19075624644756317]
  std: [0.22165445983409882, 0.22165445983409882, 0.22165445983409882]
  input_size: 496
  push_with_augmentation: False
  data_augmentation: # available operations are list in 'data_augmentation_args' below
    - random_crop
    - horizontal_flip
    - vertical_flip
    - color_distortion
    - rotation
    - translation

train:
  base_architecture: 'resnet50' # ['bagnet33', 'resnet50']
  epochs: 201
  train_last_layer_epoch: 10 # 20
  warm_epochs: 10 # set to 0 to disable warmup
  push_start: 10 # from 0 to 9 = 10 epochs
  batch_size: 16 # 16, 8, 32 for OCT
  num_workers: 24 # 24
  criterion: cross_entropy
  pretrained: True  ## always
  build_backbone: False # if True built the backbone bagnet from scratch otherwise use build_bagnet_model function
  pin_memory: true # enables fast data transfer to CUDA-enabled GPUs
  class_specific: True
  fc_classification_layer: False # False:'SA', True:'FCL'
  random_SA_init: False
  fan_out_sa_init: False
  reg_fcl: True # False or True (value is solver.l1)
  reg_proto: -0.005 #-0.005 # 0.5, 0.05, 0.005, False => enforce the dissimilarity between proto
  reg: 0.04 #0.04  # 0.05 ~ reg similarity map
  reg_epoch: False # False, 51

prototype:
  shape: [10, 512, 1, 1] # (prototype_nber=nber_class x 10, out_channel={128, 256, 512}, size, size) 
  activation_function: 'log' # 'linear' prototype activation function could be a generic function that converts distance to similarity score  
  topk: 5 #False, 5
  add_on_layers_type: 'regular'
  proto_bound_boxes_filename_prefix: 'bb'
  prototype_img_filename_prefix: 'prototype-img'
  prototype_self_act_filename_prefix: 'prototype-self-act'
  percentile_based_cropping: False # True if bagnet and False otherwise ~ not yet used
  prototype_layer_stride: 1 # must be equal to the patch size with BagNet ~ not yet used
  
solver:
  optimizer: SGD # SGD/ADAM
  learning_rate: [0.0007, 0.001, 0.003, 0.003] # initial learning rate [0.0007, 0.001, 0.003, 0.003][backone, 1x1c_onv, proto, FCL]
  lr_scheduler: step_lr # [cosine, clipped_cosine] available schedulers are list in 'scheduler_args' below. please remember to update scheduler_args when number of epoch changes.
  momentum: 0.9 # only for SGD. set to 0 to disable momentum
  nesterov: true # only for SGD.
  weight_decay: [0.0005, 0.001] # set to 0 to disable weight decay
  coefs: # weighting of different training losses 
    crs_ent: 1
    clst: 0.8
    sep: -0.08
    l1: 0.0001 #1e-4 = 0.0001 => could be 0.001 with SA
  warm_solver: # (added conv layers, proto layer)
    learning_rate: [0.001, 0.003]  # 
    weight_decay: [0.001, 0] #0.0005

# please refer to documents of torch.optim
scheduler_args:
  exponential:
    gamma: 0.6 # multiplicative factor of learning rate decay
  step_lr:
    gamma: 0.1
    step_size: 5
  multiple_steps:
    milestones: [15, 25, 45]
    gamma: 0.1 # multiplicative factor of learning rate decay
  cosine:
    T_max: 50 # maximum number of iterations
    eta_min: 0 # minimum learning rate
  reduce_on_plateau:
    mode: min
    factor: 0.1 # new learning rate = factor * learning rate
    patience: 5 # number of epochs with no improvement after which learning rate will be reduced.
    threshold: 0.0001 # threshold for measuring the new optimum
    eps: 0.00001 # minimal decay applied to learning rate
  clipped_cosine:
    T_max: 50
    min_lr: 0.0001 #
  
oct_data_augmentation_args:
  horizontal_flip:
    prob: 0.5
  vertical_flip:
    prob: 0.5
  color_distortion:
    prob: 0.5
    brightness: 0.2
    contrast: 0.2
    saturation: 0.2
    hue: 0.1
  random_crop: # randomly crop and resize to input_size
    prob: 0.5
    scale: [0.87, 1.15] # range of size of the origin size cropped
    ratio: [0.7, 1.3] # range of aspect ratio of the origin aspect ratio cropped
  rotation:
    prob: 0.5
    degrees: [-180, 180]
  translation:
    prob: 0.5
    range: [0.2, 0.2]
  value_fill: 0 # NOT a data augmentation operation. pixel fill value for the area outside the image
